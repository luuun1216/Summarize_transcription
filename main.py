# -*- coding: utf-8 -*-
"""side_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGoGQFBTqE_lciEfvjsPZLQ1-JFoQgW3
"""


!pip install transformers accelerate -q
!pip install beautifulsoup4 requests -q


from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime

# 使用 Hugging Face 的 Qwen 小模型
model_name = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map="auto")

# 使用 text-generation pipeline
summarizer = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)

def debug_selectors(soup):
    """
    列印多組 selector 的擷取結果，主要是我用來確認有沒有抓到網頁中正確的文字區塊位置，這個主要是我來測試的。
    """
    selectors = [
        (".speech__content", soup.select(".speech__content")),
        (".speech__content p", soup.select(".speech__content p")),
        (".speech-wrapper p", soup.select(".speech-wrapper p")),
    ]
    print("\n--- Selector Debug Info ---")
    for sel, result in selectors:
        print(f"Selector '{sel}': {len(result)} elements")

def extract_transcription_text(url):
    """
    根據指定 URL 下載 HTML，並擷取轉錄文字內容。
    擷取 .speech__content p 標籤下的段落文字。
    """
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    debug_selectors(soup)
    content_blocks = soup.select(".speech__content p")
    print(f"\n 共擷取到 {len(content_blocks)} 段落")
    transcript = "\n".join([t.get_text(strip=True) for t in content_blocks])
    return transcript

def clean_text(text):
    """
    做資料清洗，利用re library將抓取到的轉錄文字清理成單行格式：
    - 移除多餘的換行符號與空白
    - 轉為連續字串方便模型閱讀
    """
    text = text.replace('\n', ' ')
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def build_prompt(cleaned_text):
    """
    根據清理後的文字建立模型輸入的 prompt。
    引導模型生成摘要與結論。
    """
    prompt = (
        f"這是一段會議紀錄：{cleaned_text}\n\n"
        "請你根據這段內容，簡單寫出摘要與結論。\n\n"
    )
    print("\n--- Prompt Preview ---")
    print(prompt[:1000] + ("..." if len(prompt) > 1000 else ""))
    return prompt

def summarize_transcription(text):
    """
    總結會議轉錄文字內容：
    1. 清理原始文字
    2. 建立 prompt 並送入模型
    3. 擷取摘要與結論部分
    """
    cleaned = clean_text(text)[:500] #可以調整要餵給模型的文字數量，我這邊設定500個字。
    prompt = build_prompt(cleaned)

    # 呼叫 LLM 生成文字
    generated = summarizer(prompt)[0]['generated_text']
    # 擷取摘要與結論內容，根據「摘要：」與「結論：」分段。
    summary_part = generated.split("摘要：")[-1]
    conclusion_split = summary_part.split("結論：")
    summary = conclusion_split[0].strip()
    conclusion = conclusion_split[1].strip() if len(conclusion_split) > 1 else ""
    return {
        "summary": summary,
        "conclusion": conclusion
    }

# 測試用範例網址
CN_sample_url = "https://sayit.archive.tw/2025-02-02-bbc-%E6%8E%A1%E8%A8%AA"
EN_sample_url = "https://sayit.archive.tw/2025-04-03-interview-with-polly-curtis"

print("Downloading transcription text...")
transcription_text = extract_transcription_text(EN_sample_url)
print("Summarizing with Qwen 1.8B...")
summary_result = summarize_transcription(transcription_text)

#  輸出結果為 JSON 格式
print("\n--- Summary Output ---")
print(summary_result.get("summary", ""))

# print("\n--- Full Output (JSON) ---")
# print(json.dumps(summary_result, indent=2, ensure_ascii=False))

# # 寫入 JSON 檔案供下載
filename = f"summary_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
with open(filename, "w", encoding="utf-8") as f:
    json.dump(summary_result, f, ensure_ascii=False, indent=2)

print(f"\n JSON 檔案已儲存為: {filename}")
