# -*- coding: utf-8 -*-
"""side_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGoGQFBTqE_lciEfvjsPZLQ1-JFoQgW3
"""

!pip install transformers accelerate -q
!pip install beautifulsoup4 requests -q

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime

# 模型設定：使用 Hugging Face 的 Qwen 小模型
model_name = "Qwen/Qwen1.5-1.8B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map="auto")

# 使用 text-generation pipeline
summarizer = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)

def debug_selectors(soup):
    selectors = [
        (".speech__content", soup.select(".speech__content")),
        (".speech__content p", soup.select(".speech__content p")),
        (".speech-wrapper p", soup.select(".speech-wrapper p")),
    ]
    print("\n--- Selector Debug Info ---")
    for sel, result in selectors:
        print(f"Selector '{sel}': {len(result)} elements")

def extract_transcription_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    debug_selectors(soup)
    content_blocks = soup.select(".speech__content p")
    print(f"\n 共擷取到 {len(content_blocks)} 段落")
    transcript = "\n".join([t.get_text(strip=True) for t in content_blocks])
    return transcript

def clean_text(text):
    text = text.replace('\n', ' ')
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def build_prompt(cleaned_text):
    prompt = (
        f"這是一段會議紀錄：{cleaned_text}\n\n"
        "請你根據這段內容，簡單寫出摘要與結論。\n\n"
    )
    print("\n--- Prompt Preview ---")
    print(prompt[:1000] + ("..." if len(prompt) > 1000 else ""))
    return prompt

def summarize_transcription(text):
    cleaned = clean_text(text)[:500]
    prompt = build_prompt(cleaned)
    generated = summarizer(prompt)[0]['generated_text']
    summary_part = generated.split("摘要：")[-1]
    conclusion_split = summary_part.split("結論：")
    summary = conclusion_split[0].strip()
    conclusion = conclusion_split[1].strip() if len(conclusion_split) > 1 else ""
    return {
        "summary": summary,
        "conclusion": conclusion
    }

# 測試用範例網址
CN_sample_url = "https://sayit.archive.tw/2025-02-02-bbc-%E6%8E%A1%E8%A8%AA"
EN_sample_url = "https://sayit.archive.tw/2025-04-03-interview-with-polly-curtis"

print("Downloading transcription text...")
transcription_text = extract_transcription_text(EN_sample_url)
print("Summarizing with Qwen 1.8B...")
summary_result = summarize_transcription(transcription_text)

#  輸出結果為 JSON 格式（可讀）
print("\n--- Summary Output ---")
print(summary_result.get("summary", ""))

# print("\n--- Full Output (JSON) ---")
# print(json.dumps(summary_result, indent=2, ensure_ascii=False))

# # 寫入 JSON 檔案供下載
filename = f"summary_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
with open(filename, "w", encoding="utf-8") as f:
    json.dump(summary_result, f, ensure_ascii=False, indent=2)

# print(f"\n JSON 檔案已儲存為: {filename}")
